
<head>
<style type="text/css">
.auto-style1 {
	font-size: x-large;
}
</style>
</head>

<ul>
<li><a href="#Introduction">Introduction</a></li>

<li><a href="#What-Is-Spark-/-Databricks?">What Is Spark / Databricks?</a></li>

<li><a href="#Why-Is-Databricks-So-Cool?">Why Is Databricks So Cool?</a></li>

<li><a href="#Where-Is-The-Code?">Where Is The Code?</a></li>

<li><a href="#Prerequisites">Prerequisites</a></li>

<li><a href="#Getting-Started-With-Databricks-In-Azure">Getting Started With Databricks In Azure</a>
<ul>
<li><a href="#Exploring-The-Workspace">Exploring The Workspace</a></li>

<li><a href="#Create-A-Cluster">Create A Cluster</a></li>

<li><a href="#Explore-A-NoteBook">Explore A NoteBook</a></li>

<li><a href="#Run-Some-NoteBook-Code">Run Some NoteBook Code</a></li>

<li><a href="#Read/Write-From-Azure-Blob-Store">Read/Write From Azure Blob Store</a></li>

<li><a href="#Creating-Jobs-In-DataBricks-Web-UI">Creating Jobs In DataBricks Web UI</a></li>

<li><a href="#Launch-The-Spark-UI/Dashboard">Launch The Spark UI/Dashboard</a></li>

</ul>

<li><a href="#Databricks-API">Databricks API</a>
<ul>
<li><a href="#What-APIs-Are-Available?">What APIs Are Available?</a></li>

<li><a href="#Creating-A-Token-For-Usage-With-API">Creating A Token For Usage With API</a></li>

<li><a href="#Common-Stuff-You-Need-To-Do-With-Any-API-Call">Common Stuff You Need To Do With Any API Call</a>
<ul>
<li><a href="#Creating-A-Base64-Encoded-Token-Value">Creating A Base64 Encoded Token Value</a></li>

</ul>

<li><a href="#A-Simple-Throwaway-Demo-App">A Simple Throwaway Demo App</a>
<ul>
<li><a href="#How-Do-I-set-My-Token-For-The-Demo-App?">How Do I set My Token For The Demo App?</a></li>

<li><a href="#Example-ViewModel">Example ViewModel</a></li>

<li><a href="#DataTemplate-For-UI">DataTemplate For UI</a></li>

</ul>

<li><a href="#Cluster-API-Exploration">Cluster API Exploration</a>
<ul>
<li><a href="#Clusters-List">Clusters List</a></li>

<li><a href="#Cluster-Get">Cluster Get</a></li>

<li><a href="#Cluster-Start">Cluster Start</a></li>

</ul>

<li><a href="#Jobs-API-Exploration">Jobs API Exploration</a>
<ul>
<li><a href="#Jobs-List">Jobs List</a></li>

<li><a href="#Jobs-Create">Jobs Create</a></li>

	<li><a href="#Jobs-Runs-Get">Jobs Runs Get</a></li>

<li><a href="#Jobs-Run-Now">Jobs Run Now</a></li>

<li><a href="#Jobs-Run-Submit">Jobs Run Submit</a>
<ul>
<li><a href="#The-Scala-Project">The Scala Project</a></li>

</ul>

</ul>

</ul>

<li><a href="#Conclusion">Conclusion</a></li>

</ul>

<h1><a name="Introduction" id="Introduction">Introduction</a></h1>
<p>This article I will be talking about <a href="https://spark.apache.org/">Apache 
Spark</a> / <a href="https://databricks.com/">Databricks</a>. It will guide you 
through how to use the new cloud based hosted <a href="https://databricks.com/">Databricks</a> platform. This article 
will talk about it from a Microsoft Azure standpoint but it should be exactly 
the same ideas if you were to use Amazon cloud.</p>
<p>&nbsp;</p>
<h1><a name="What-Is-Spark-/-Databricks?" id="What-Is-Spark-/-Databricks?">What Is Spark / Databricks?</a></h1>
<p>A picture says a thousand word, they say, so here is a nice picture of what 
Apache Spark is.</p>
<p><a href="WhatIsSparkBig.png" target="_blank"><img alt="" height="411" src="WhatIsSparkSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>At its heart <a href="https://spark.apache.org/">Apache 
Spark</a>&nbsp; is a tool for Data Scientists to allow them to explore and 
crunch vaste amounts of data with ease. It features things like</p>
<ul>
	<li>Distributed Datasets (so that number crunching can happen across a 
	compute cluster)</li>
	<li>A DataFrame API, such that you can do things like add columns, aggregate 
	column values, and alias data, join DataFrames. This also supports a SQL 
	syntax, and also rund distrobuted across a cluster</li>
	<li>A streaming API</li>
	<li>A Machine Learning API</li>
	<li>A Graph API</li>
</ul>
<p>&nbsp;</p>
<p>Spark also comes with various adaptors to allow it connect to various data 
sources such as</p>
<ul>
	<li>Blobs</li>
	<li>Databases</li>
	<li>Filesystems (HDFS / s3 / Azure storage / azure datalake / Databricks 
	file system)</li>
</ul>
<p>&nbsp;</p>
<p>This is not the first time I have written about <a href="https://spark.apache.org/">Apache 
Spark</a>, here are some older articles on it should you be interested</p>
<p>&nbsp;</p>
<ul>
	<li>
	<a href="https://www.codeproject.com/Articles/1023037/Introduction-to-Apache-Spark">
	Introduction to Apache Spark</a></li>
	<li>
	<a href="https://www.codeproject.com/Articles/1073158/Apache-Spark-Cassandra-of">
	Apache Spark/Cassandra 1 of 2</a></li>
	<li>
	<a href="https://www.codeproject.com/Articles/1077934/Apache-Spark-Cassandra-of">
	Apache Spark/Cassandra 2 of 2</a></li>
</ul>
<p>&nbsp;</p>
<p>So when I wrote those articles, there was limited options about how you could 
run you <a href="https://spark.apache.org/">Apache 
Spark</a> jobs on a cluster, you could basically do one of the following:</p>
<ul>
	<li>Create a Java/Scala/Python app that used the <a href="https://spark.apache.org/">Apache 
Spark</a> APIs and would run against a cluster</li>
	<li>Create a JAR that you could get an existing cluster to run using a
	<strong>spark-submit</strong> command line</li>
</ul>
<p>&nbsp;</p>
<p>The problem with this was that neither were ideal, with the app approach you 
didnt really want your analytics job to be an app, you really just wanted it to 
be a class library of some sort</p>
<p>&nbsp;</p>
<p>Spark-Submit would let you submit a class library, however the feedback that 
you got from it was not great</p>
<p>&nbsp;</p>
<p>There was another product that came out to address this call
<a href="https://livy.incubator.apache.org/">Apache Livy</a> that was a REST API 
over <a href="https://spark.apache.org/">Apache 
Spark</a> . But it too had its issues, in that it was not that great to setup, 
and the API was fairly limited. To address this the good folk that own/maintain 
Apache Spark came out with <a href="https://databricks.com/">Databricks</a>.</p>
<p>&nbsp;</p>
<p><a href="https://databricks.com/">Databricks</a> is essentially a fully managed <a href="https://spark.apache.org/">Apache 
Spark</a>&nbsp; in the Cloud (Amazon / Azure). It also has the concept of REST 
APIs for common things. Lets dig into that next.</p>
<p>&nbsp;</p>
<h1><a name="Why-Is-Databricks-So-Cool?" id="Why-Is-Databricks-So-Cool?">Why Is Databricks So Cool?</a></h1>
<p>Just before we get into <a href="https://databricks.com/">Databricks</a>, why 
is it that I think it's so cool?</p>
<p>Well I have stated one point above already, but lets see the full list</p>
<ul>
	<li>Great (really good) REST API</li>
	<li>Nice managemanent dashboard in the cloud</li>
	<li>The ability to create a on-demand cluster for your own job run that is 
	torn down at the end of the job is <span class="auto-style1">MASSIVE</span>. 
	The reason this one point alone should make you think about examining <a href="https://databricks.com/">Databricks</a> 
	is as follows<ul>
		<li>By using your own cluster you are not sharing any resources with 
		some one else, so you can guarentee your performance based on the nodes 
		you choose for your cluster</li>
		<li>The cluster gets torn down after your job has run. This saves you 
		money</li>
	</ul>
	</li>
	<li>If you chose to use a single cluster rather than a <strong>new</strong> 
	cluster per job, you can have the single static cluster set to <strong>
	AutoScale</strong>. This is a pretty neat feature. Just imagine trying to do 
	that in-premise. Obviously since<a href="https://spark.apache.org/">Apache 
Spark</a> <a href="https://spark.apache.org/docs/latest/running-on-kubernetes">
	also supports running on Kubernetes</a> that does ease the process a bit. 
	However to my mind to make the best use of Kubernetes you also want to run 
	that on a Cloud such as Amazon / Azure / Google, as scaling the VMs needed 
	for a cluster is just so MUCH easier if you are in a Cloud. Just as an aside 
	if you don't know your Kubernetes from an onion I wrote a mini series on it 
	which you can find here :
	<a href="https://sachabarbs.wordpress.com/kubernetes-series/">
	https://sachabarbs.wordpress.com/kubernetes-series/</a></li>
	<li>It's actually fairly cheap I think for what it gives you</li>
	<li>It's highly intuitive</li>
</ul>
<p>&nbsp;</p>
<h1><a name="Where-Is-The-Code?" id="Where-Is-The-Code?">Where Is The Code?</a></h1>
<p>So there is a small bit of code that goes with this article, which is split 
into 2 things</p>
<ol>
	<li>A throw-away WPF app that simply acts as a vehicle to demonstrate the 
	REST calls, in all seriousness you could use postman for the <a href="https://databricks.com/">Databricks</a> 
	REST exploration. The WPF app just makes this easier, as you don't have to 
	worry about remembering to set an access token, once you have set it up 
	once, and trying to find the right REST API syntax. The UI just shows you a 
	working set of REST calls for <a href="https://databricks.com/">Databricks</a></li>
	<li>A simple IntelliJ IDEA Scala/SBT project, that represents a <a href="https://spark.apache.org/">Apache 
Spark</a> job that we wish to upload and run on <a href="https://databricks.com/">Databricks</a>. 
	This will compiled using SBT, as such SBT is a must have if you want to run 
	this code</li>
</ol>
<p>The code repo can be found here :
<a href="https://github.com/sachabarber/databrick-azure-spark-demo">
https://github.com/sachabarber/databrick-azure-spark-demo</a></p>
<p>&nbsp;</p>
<h1><a name="Prerequisites" id="Prerequisites">Prerequisites</a></h1>
<p>There are a few of these, not because <a href="https://databricks.com/">Databricks</a> 
needs them as such, but because I was keen to show you an entire workflow of how 
you might use <a href="https://databricks.com/">Databricks</a> for real to do 
your own jobs, which means we should create a new JAR file to act as a job to 
send to <a href="https://databricks.com/">Databricks</a>.</p>
<p>As such the following is required</p>
<ul>
	<li>A <a href="https://databricks.com/">Databricks</a> installation (either 
	Amazon/Azure hosted)</li>
	<li>Visual Studio 2017 (Community editition is fine here)</li>
	<li>Java 1.8 SDK installed and in your path</li>
	<li>IntelliJ IDEA Community Edition 2017.1.3 (or later)</li>
	<li>SBT Intellij IDEA plugin installed</li>
</ul>
<p>Obviously if you just want to read along and not try this yourself, you won't 
need any of this</p>
<p>&nbsp;</p>
<h1><a name="Getting-Started-With-Databricks-In-Azure" id="Getting-Started-With-Databricks-In-Azure">Getting Started With Databricks In Azure</a></h1>
<p>As I said already I will be using Microsoft Azure, but after the initial 
creation of <a href="https://databricks.com/">Databricks</a> in the Cloud (which 
will be cloud vendor specific) the rest of the instructions should hold for 
Amazon or Azure.</p>
<p>&nbsp;Anyway the first step for working with <a href="https://databricks.com/">Databricks</a> 
in the cloud is to create a new <a href="https://databricks.com/">Databricks</a> 
instance. Which for Azure simple means creating a new resource as follows:</p>
<p><a href="NewBig.png" target="_blank"><img alt="" height="354" src="NewSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>Once you have creating the <a href="https://databricks.com/">Databricks</a> 
instance, you should be able to launch the workspace from the overview of the <a href="https://databricks.com/">Databricks</a> 
instance</p>
<p>&nbsp;</p>
<p><a href="OverviewBig.png" target="_blank"><img alt="" height="354" src="OverviewSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>This should launch you into a new <a href="https://databricks.com/">Databricks</a> 
workspace website that is coupled to your Azure/Amazon subscription, so you 
should initially see something like this after you have passed the logging in 
phase (which happens automatically, well on Azure it does anyway)</p>
<p>&nbsp;</p>
<p><a href="WorkSpaceOverviewBig.png" target="_blank"><img alt="" height="354" src="WorkSpaceOverviewSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>From here you can see/do the following things, some of which we will explore 
in more detail below</p>
<ul>
	<li>Create a new Job/Cluster/Notebook</li>
	<li>Explore DBFS (<a href="https://databricks.com/">Databricks</a> 
	file system) data</li>
	<li>Look at previous job runs</li>
	<li>View / start / stop clusters</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2><a name="Exploring-The-Workspace" id="Exploring-The-Workspace">Exploring The Workspace</a></h2>
<p>In this section we are going to explore what you can do in the <a href="https://databricks.com/">Databricks</a> 
workspace web site that is tied to your <a href="https://databricks.com/">Databricks</a> 
cloud installation . We will not be leaving the workspace web site, everything 
below will be done directly in the workspace web site, which to my mind is 
pretty cool, but where this stuff really shines is when we get to do all of this 
stuff programatically, rather than just clicking buttons on a web site.</p>
<p>After all we would like to build this stuff into our own processing 
pipelines. Luckily there is a pretty good one-one translation from what you can 
do using the web site compared to what is exposed via the rest API. But I am 
jumping ahead we will get there in the section after this, so for now lets just 
examine what we can do in the&nbsp; <a href="https://databricks.com/">Databricks</a> 
workspace web site that is tied to your <a href="https://databricks.com/">Databricks</a> 
cloud installation.</p>
<p>&nbsp;</p>
<h2><a name="Create-A-Cluster" id="Create-A-Cluster">Create A Cluster</a></h2>
<p>So the very first thing you might like to do is create a cluster to run some 
code on. This is easily done using the <a href="https://databricks.com/">Databricks</a> 
workspace web site as follows</p>
<p><img alt="" height="741" src="CreateCluster.png" width="400"></p>
<p>&nbsp;</p>
<p>This will bring you to a screen like this, where you can configure your 
cluster, where you pick the variouos bits and peices for your cluster</p>
<p><a href="CreateCluster2Big.png" target="_blank"><img alt="" height="354" src="CreateCluster2Small.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>Once you have created a cluster it will end up being listed on the clusters 
overview page</p>
<p><a href="clustersOverviewBig.png" target="_blank"><img alt="" height="354" src="clustersOverviewSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<ul>
	<li>Interactive clusters are ones that are tied to Notebooks which we look 
	at next</li>
	<li>Job clusters are ones that are used to run Jobs</li>
</ul>
<p>&nbsp;</p>
<h2><a name="Explore-A-NoteBook" id="Explore-A-NoteBook">Explore A NoteBook</a></h2>
<p>Now I am a long time Dev (I'm old, or feel it or something), so think nothing 
about opening up an IDE and writing an app/class lib/jar whatever. Bust at its 
heart <a href="https://spark.apache.org/">Apache 
Spark</a> is a tool for data scientists, who simply want to try some simple bits 
of number crunching code. That is exactly where notbooks come into play.</p>
<p>A notebook is a cellular editor that is hosted, that allows the user to run 
python/R/Scala code against a <a href="https://spark.apache.org/">Apache 
Spark</a> cluster.</p>
<p>You can create a new notebook from the home menu as shown below</p>
<p><a href="newNotebookBig.png" target="_blank"><img alt="" height="354" src="newNotebookSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>So after you have picked your language you will be presented with a blank 
notebook where you can write some code into the cells. Teaching you the short 
cuts and how to use notebooks properly is outside the scope of this article, but 
here is some points on notebooks:</p>
<ul>
	<li>They allow you to quickly explore the APIs</li>
	<li>They allow you to re-assign variables which are remembered</li>
	<li>They allow you to enter just a specific cell</li>
	<li>They give you some pre-defined variables. But be wary you will need to 
	swap these out if you translate this to real code. For example <code>spark
	</code>is a pre-defined variable</li>
</ul>
<p>&nbsp;</p>
<h2><a name="Run-Some-NoteBook-Code" id="Run-Some-NoteBook-Code">Run Some NoteBook Code</a></h2>
<p>So lets say I have just created a Scala notebook, and I typed the text as 
shown below in a cell. I can quickly run this using ALT&nbsp; + Enter or the run 
button in the notebook UI</p>
<p>&nbsp;</p>
<p><a href="cellBig.png" target="_blank"><img alt="" height="354" src="cellSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<p>What this will do is run the active cell and print out the variables/output 
statements to the notebook UI, which can also be seen above</p>
<p>&nbsp;</p>
<h2><a name="Read/Write-From-Azure-Blob-Store" id="Read/Write-From-Azure-Blob-Store">Read/Write From Azure Blob Store</a></h2>
<p>One of the very common tasks when using <a href="https://spark.apache.org/">Apache 
Spark</a> is to grab some data from some external source and save it to storage 
once transformed into the required results</p>
<p>&nbsp;</p>
<p>Here is an example of working with an existing Azure Blob Storage Account and 
some Scala code. This particular example simply loads a CSV file from Azure Blob 
Storage transforms the file and then saves it back to Azure Blob Storage as a 
date stamped named CSV file</p>
<p>&nbsp;</p>
<pre lang="scala">
import java.util.Calendar
import java.text.SimpleDateFormat

spark.conf.set("fs.azure.account.key.YOUR_STORAGE_ACCOUNT_NAME_HERE.blob.core.windows.net", "YOUR_STORAGE_KEY_HERE")

spark.sparkContext.hadoopConfiguration.set(
  "fs.azure.account.key.YOUR_STORAGE_ACCOUNT_NAME_HERE.blob.core.windows.net",
  "YOUR_STORAGE_KEY_HERE"
)


val now = Calendar.getInstance().getTime()
val minuteFormat = new SimpleDateFormat("mm")
val hourFormat = new SimpleDateFormat("HH")
val secondFormat = new SimpleDateFormat("ss")

val currentHour = hourFormat.format(now)      // 12
val currentMinute = minuteFormat.format(now)  // 29
val currentSecond = secondFormat.format(now)           // PM
val fileId  = currentHour  + currentMinute + currentSecond
fileId

val data = Array(1, 2, 3, 4, 5)
val dataRdd = sc.parallelize(data)
val ages_df = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("wasbs://YOUR_CONTAINER_NAME_HERE@YOUR_STORAGE_ACCOUNT_NAME_HERE.blob.core.windows.net/Ages.csv")
ages_df.head

//https://github.com/databricks/spark-csv 
val selectedData = ages_df.select("age")
selectedData.write
    .format("com.databricks.spark.csv")
    .option("header", "false")
    .save("wasbs://YOUR_CONTAINER_NAME_HERE@YOUR_STORAGE_ACCOUNT_NAME_HERE.blob.core.windows.net/" + fileId + "_SavedAges.csv")


val saved_ages_df = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("wasbs://YOUR_CONTAINER_NAME_HERE@YOUR_STORAGE_ACCOUNT_NAME_HERE.blob.core.windows.net/" + fileId + "_SavedAges.csv")

saved_ages_df.show()
</pre>
<p>&nbsp;</p>
<p>Which for me looked like this for my storage account in Azure</p>
<p><a href="storageBig.png" target="_blank"><img alt="" height="289" src="storageSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>If you are wondering why there are folders for the results such as 
080629_SavedAges.csv, this is due to how <a href="https://spark.apache.org/">Apache 
Spark</a> deals with partitions. Trust me it doesnt matter when you load things 
back into memory as <a href="https://spark.apache.org/">Apache 
Spark</a> just deals with this, as can be seen </p>
<p>&nbsp;</p>
<p><a href="partBig.png" target="_blank"><img alt="" height="308" src="partSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<h2><a name="Creating-Jobs-In-DataBricks-Web-UI" id="Creating-Jobs-In-DataBricks-Web-UI">Creating Jobs In DataBricks Web UI</a></h2>
<p>The <a href="https://databricks.com/">Databricks</a> 
web UI allows you to create a new job from either a Notebook or a JAR that you 
have that you can drag in and set a main entry point for. You may also setup a 
schedule and a cluster that you want to use. Once you are happy you can click 
the &quot;<strong>run now</strong>&quot; button which will run your job.</p>
<p><a href="CReateJobBig.png" target="_blank"><img alt="" height="354" src="CReateJobSmall.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<h2><a name="Launch-The-Spark-UI/Dashboard" id="Launch-The-Spark-UI/Dashboard">Launch The Spark UI/Dashboard</a></h2>
<p>Once you have run a job it is likely that you want to have a look at it to 
see that it worked how you expected it to work, and that it is running 
optimally. Luckily <a href="https://spark.apache.org/">Apache 
Spark</a> comes equipt with a nice visualiser for a given analysis run that you 
can use for this. Its kind of like the SQL Query Profiler in SQL Server.</p>
<p>This is accessible from the jobs page</p>
<p><a href="JobsBig.png" target="_blank"><img alt="" height="354" src="Jobs.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>So lets take a look at a successful job, which we can view using the &quot;<strong>Succeeded</strong>&quot; 
link in the <strong>Last Run</strong> column in the <strong>Jobs</strong> page.</p>
<p>From there we can view the <a href="https://spark.apache.org/">Apache 
Spark</a> UI or the logs for the job.</p>
<p>Lets see the <a href="https://spark.apache.org/">Apache 
Spark</a> UI for this job.</p>
<p><a href="SparkUIBig.png" target="_blank"><img alt="" height="354" src="SparkUI.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>The Spark UI is your friend, try and get acquianted with it</p>
<p>&nbsp;</p>
<h1><a name="Databricks-API" id="Databricks-API">Databricks API</a></h1>
<p>Ok so now that we have covered how to use the <a href="https://databricks.com/">Databricks</a> 
web UI, how about we get familiar with the REST API such that we can craft our 
own code around using <a href="https://spark.apache.org/">Apache 
Spark</a> as our analytics engine. This next section will show how to use some 
of the REST APIs available.</p>
<p>&nbsp;</p>
<h2><a name="What-APIs-Are-Available?" id="What-APIs-Are-Available?">What APIs Are Available?</a></h2>
<p>So you may now be wondering what APIs are actually available? This is the 
place to check : <a href="https://docs.databricks.com/api/latest/index.html">
https://docs.databricks.com/api/latest/index.html</a></p>
<p>From there the main top level APIs are</p>
<ul>
	<li>Clusters</li>
	<li>DBFS</li>
	<li>Groups</li>
	<li>Instance Profiles</li>
	<li>Jobs</li>
	<li>Libraries</li>
	<li>Secrets</li>
	<li>Token</li>
	<li>Workspace</li>
</ul>
<p>There are simply not enough hours in the day for me to show you ALL of them. 
So I have chosen a few to demo, which we will talk about below</p>
<p>&nbsp;</p>
<h2><a name="Creating-A-Token-For-Usage-With-API" id="Creating-A-Token-For-Usage-With-API">Creating A Token For Usage With API</a></h2>
<p>The <a href="https://databricks.com/">Databricks</a> 
REST APIs ALL need to have a JWT token associated with them. Which means you 
need to firstly create a token for this. This is easily achieved in the <a href="https://databricks.com/">Databricks</a> 
web UI. Just follow these steps</p>
<p><a href="DatabricksAzureCreateTokenBig.png" target="_blank"><img alt="" height="338" src="DatabricksAzureCreateToken.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>So once you have done that, grab the token value, and you will also need to 
take a note of one other bit of information which is shown highlighted in the 
image below. With these 2 bits of information we can use Postman to try a 
request</p>
<p><a href="workspaceInAzureBig.png" target="_blank"><img alt="" height="112" src="workspaceInAzure.png" width="630"><a/></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>&nbsp;</p>
<h2><a name="Common-Stuff-You-Need-To-Do-With-Any-API-Call" id="Common-Stuff-You-Need-To-Do-With-Any-API-Call">Common Stuff You Need To Do With Any API Call</a></h2>
<p>So as I just said you will need to ensure that the token from the previous 
step is supplied on every call. But just how do we do that? What does it look 
like? Lets use Postman to show an example using the last 2 bits of information 
from the previous pararaph</p>
<h3><a name="Creating-A-Base64-Encoded-Token-Value" id="Creating-A-Base64-Encoded-Token-Value">Creating A Base64 Encoded Token Value</a></h3>
<p>The token you got from above needs to be turned into a Base64 encoded string. 
There are many online tools for this, just pick one. The important thing to note 
is that you must <strong>ALSO</strong> include a prefix of <code>&quot;token:&quot;</code>. So the full 
string to encode is something like <code>&quot;token:dapi56b...........d5&quot;</code></p>
<p>&nbsp;</p>
<p><img alt="" height="797" src="gettingTokenHeader.png" width="621"></p>
<p>This will give you a base64 encoded string. From there we need to head into 
Postman to try out the request, which may look somethinng like this</p>
<p><a href="testRequestInPostManBig.png" target="_blank"><img alt="" height="280" src="testRequestInPostMan.png" width="630"></a></p>
<p><em>CLICK FOR BIGGER IMAGE</em></p>
<p>The important things to note here are:</p>
<ul>
	<li>We create a header : <code>Key = Authorization, Value = Basic 
	YOUR_BASE64_ENCODED_STRING_FROM_PREVIOUS_STEP</code></li>
	<li>We use the information from the Azure/AWS portal to use as part of the 
	Uri. So for this is a valid erquest for my <a href="https://databricks.com/">Databricks</a> 
	Azure installation <code>
	https://northeurope.azuredatabricks.net/api/2.0/clusters/spark-versions</code></li>
</ul>
<p>&nbsp;</p>
<h2><a name="A-Simple-Throwaway-Demo-App" id="A-Simple-Throwaway-Demo-App">A Simple Throwaway Demo App</a></h2>
<p>Obviously you could just mess around in Postman to learn how the <a href="https://databricks.com/">Databricks</a> 
REST APIs work, nothing wrong with that at all. But to make life easier for you 
I have come up with a simple (throw away) demo app that you can use to explore 
what I think are the 2 most important APIs</p>
<p>This is what it looks like running</p>
<p><img alt="" height="354" src="DemoAppBig.png" width="630"></p>
<p>And here is what it looks like when you have chosen to run one of the 
pre-canned REST API calls that the demo app provides</p>
<p>&nbsp;</p>
<p><img alt="" height="354" src="DemoAppGetClusters.png" width="630"></p>
<p>&nbsp;</p>
<h3><a name="How-Do-I-set-My-Token-For-The-Demo-App?" id="How-Do-I-set-My-Token-For-The-Demo-App?">How Do I set My Token For The Demo App?</a></h3>
<p>So above when I started talking about the <a href="https://databricks.com/">Databricks</a> 
REST APIs we said we needed to supply an API token. So how does the demo app 
deal with this. </p>
<p>Well there are 2 parts to how it does that, this entry in the App.Config 
should point to your own file that contains the token information</p>
<p><img alt="" height="393" src="ConfigBig.png" width="630"></p>
<p>So for me this is my file</p>
<p>
<a href="file:///C:/Users/sacha/Desktop/databrick-azure-spark-demo/MyDataBricksToken.txt">
C:\Users\sacha\Desktop\databrick-azure-spark-demo\MyDataBricksToken.txt</a></p>
<p>Where the file simply contains a single line of contents <a/> <code>&quot;token:dapi56b...........d5&quot;</code></a> 
which is the base64 encoded string proceeded by <code>&quot;token:&quot;</code> which we 
talked about above.</p>
<p>This is then read into a globally available property in the demo app as 
follows:</p>
<pre lang="csharp">
using System.Configuration;
using System.IO;
using System.Windows;
using SAS.Spark.Runner.REST.DataBricks;

namespace SAS.Spark.Runner
{
    public partial class App : Application
    {
        protected override void OnStartup(StartupEventArgs e)
        {
            base.OnStartup(e);

            var tokenFileName = ConfigurationManager.AppSettings["TokenFileLocation"];
            if (!File.Exists(tokenFileName))
            {
                MessageBox.Show("Expecting token file to be provided");
            }

            AccessToken = File.ReadAllText(tokenFileName);

            if(!AccessToken.StartsWith("token:"))
            {
                MessageBox.Show("Token file should start with 'token:' + 
				  "following directly by YOUR DataBricks initial token you created");
            }
        }

        public static string AccessToken { get; private set; }
    }
}
</pre>
<p>&nbsp;</p>
<p>And that is all there is to it. The demo app should take care of the rest of 
it for you.</p>
<p>&nbsp;</p>
<p>As I say I did not have time to explore every single API, but I had time to 
look at 2 of the most common ones, Clusters and Jobs. Which I will talk about 
below.</p>
<p>But before I get into that, I just wanted to show you the rough idea behind 
each of the API explorations</p>
<p>&nbsp;</p>
<h3><a name="Example-ViewModel" id="Example-ViewModel">Example ViewModel</a></h3>
<p>Most of the API explorations are done using a viewmodel something like this</p>
<pre lang="csharp">
using SAS.Spark.Runner.REST.DataBricks;
using System;
using System.Threading.Tasks;
using System.Windows.Input;
using SAS.Spark.Runner.Services;

namespace SAS.Spark.Runner.ViewModels.Clusters
{
    public class ClusterGetViewModel : INPCBase
    {
        private IMessageBoxService _messageBoxService;
        private IDatabricksWebApiClient _databricksWebApiClient;
        private string _clustersJson;
        private string _clusterId;

        public ClusterGetViewModel(
            IMessageBoxService messageBoxService,
            IDatabricksWebApiClient databricksWebApiClient)
        {
            _messageBoxService = messageBoxService;
            _databricksWebApiClient = databricksWebApiClient;
            FetchClusterCommand = 
				new SimpleAsyncCommand&lt;object, object&gt;(ExecuteFetchClusterCommandAsync);
        }

        private async Task&lt;object&gt; ExecuteFetchClusterCommandAsync(object param)
        {
            if(string.IsNullOrEmpty(_clusterId))
            {
                _messageBoxService.ShowError("You must supply 'ClusterId'");
                return System.Threading.Tasks.Task.FromResult&lt;object&gt;(null);
            }

            try
            {
                var cluster = await _databricksWebApiClient.ClustersGetAsync(_clusterId);
                ClustersJson = cluster.ToString();
            }
            catch(Exception ex)
            {
                _messageBoxService.ShowError(ex.Message);
            }
            return System.Threading.Tasks.Task.FromResult&lt;object&gt;(null);
        }


        public string ClustersJson
        {
            get
            {
                return this._clustersJson;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._clustersJson, 
					value, () =&gt; ClustersJson);
            }
        }

        public string ClusterId
        {
            get
            {
                return this._clusterId;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._clusterId, 
					value, () =&gt; ClusterId);
            }
        }

        public ICommand FetchClusterCommand { get; private set; }
    }
}
</pre>
<p>The idea being that we use simple REST Service and we have a property representing the JSON response. 
The REST service implements this interface</p>
<pre lang="csharp">
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json.Linq;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public interface IDatabricksWebApiClient 
    {

        //https://docs.databricks.com/api/latest/jobs.html#create
        Task&lt;CreateJobResponse&gt; JobsCreateAsync(string jsonJobRequest);

        //https://docs.databricks.com/api/latest/jobs.html#jobsrunnow
        Task&lt;DatabricksRunNowResponse&gt; JobsRunNowAsync(DatabricksRunNowRequest runRequest);

        //https://docs.databricks.com/api/latest/jobs.html#runs-get
        Task&lt;DatabricksRunResponse&gt; JobsRunsGetAsync(int runId);

        //https://docs.databricks.com/api/latest/jobs.html#list
        Task&lt;JObject&gt; JobsListAsync();

        //https://docs.azuredatabricks.net/api/latest/jobs.html#runs-submit
        Task&lt;DatabricksRunNowResponse&gt; JobsRunsSubmitJarTaskAsync(RunsSubmitJarTaskRequest runsSubmitJarTaskRequest);

        //https://docs.azuredatabricks.net/api/latest/clusters.html#start
        Task&lt;DatabricksClusterStartResponse&gt; ClustersStartAsync(string clusterId);

        //https://docs.azuredatabricks.net/api/latest/clusters.html#get
        Task&lt;JObject&gt; ClustersGetAsync(string clusterId);

        //https://docs.databricks.com/api/latest/clusters.html#list
        Task&lt;ClusterListResponse&gt; ClustersListAsync();

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#list
        Task&lt;DbfsListResponse&gt; DbfsListAsync();

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#put
        Task&lt;JObject&gt; DbfsPutAsync(FileInfo file);

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#dbfsdbfsservicecreate
        Task&lt;DatabricksDbfsCreateResponse&gt; DbfsCreateAsync(DatabricksDbfsCreateRequest dbfsRequest);

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#dbfsdbfsserviceaddblock
        Task&lt;JObject&gt; DbfsAddBlockAsync(DatabricksDbfsAddBlockRequest dbfsRequest);

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#close
        Task&lt;JObject&gt; DbfsCloseAsync(DatabricksDbfsCloseRequest dbfsRequest);
    }
}
</pre>
<p>&nbsp;</p>
<h3><a name="DataTemplate-For-UI" id="DataTemplate-For-UI">DataTemplate For UI</a></h3>
<p>The actual UI is simple done using a <code>DataTemplate</code>, where we have bound the ViewModel in question to a 
<code>ContentControl</code>. For the JSON representation I am just using the
<a href="http://avalonedit.net/">AvalonEdit TextBox</a>.</p>
<p>Here is an example for the ViewModel above:</p>
<pre lang="xml">
&lt;Controls:MetroWindow x:Class="SAS.Spark.Runner.MainWindow"
        xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
        xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
        xmlns:d="http://schemas.microsoft.com/expression/blend/2008"
        xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006"
        xmlns:vms="clr-namespace:SAS.Spark.Runner.ViewModels"
        xmlns:vmsClusters="clr-namespace:SAS.Spark.Runner.ViewModels.Clusters"
        xmlns:vmsJobs="clr-namespace:SAS.Spark.Runner.ViewModels.Jobs"
        xmlns:avalonEdit="http://icsharpcode.net/sharpdevelop/avalonedit"
        xmlns:Controls="clr-namespace:MahApps.Metro.Controls;assembly=MahApps.Metro"
        xmlns:local="clr-namespace:SAS.Spark.Runner"       
        mc:Ignorable="d"
        WindowState="Maximized"
        Title="DataBricks API Runner"&gt;
    &lt;Controls:MetroWindow.Resources&gt;
		.....
		.....
		&lt;DataTemplate DataType="{x:Type vmsClusters:ClusterGetViewModel}"&gt;
			&lt;DockPanel LastChildFill="True"&gt;
				&lt;StackPanel Orientation="Horizontal" DockPanel.Dock="Top"&gt;
					&lt;Label Content="ClusterId" Margin="3" VerticalAlignment="Center"
						VerticalContentAlignment="Center" Height="24"/&gt;
					&lt;TextBox Text="{Binding ClusterId}" Width="200" VerticalAlignment="Center"
						VerticalContentAlig
						nment="Center" Height="24"/&gt;
					&lt;Button Content="Get Cluster" Margin="3,0,3,0" Width="100" 
					HorizontalAlignment="Left"
					VerticalAlignment="Center"
					VerticalContentAlignment="Center"
					Command="{Binding FetchClusterCommand}"/&gt;
				&lt;/StackPanel&gt;
				&lt;avalonEdit:TextEditor
				FontFamily="Segoe UI"
				SyntaxHighlighting="JavaScript"
				FontSize="10pt"
				vms:TextEditorProps.JsonText="{Binding ClustersJson}"/&gt;
			&lt;/DockPanel&gt;
		&lt;/DataTemplate&gt;
	&lt;Controls:MetroWindow.Resources&gt;
&lt;/Controls:MetroWindow&gt;
</pre>
<p>&nbsp;</p>
<p>As the ViewModels used in the demo app all mainly follow this pattern, I wont 
be showing you any more ViewModel code apart from one where we upload a JAR file 
as that one is a bit special.</p>
<p>Just have in the back of your mind that all roughly work this way, and you 
will be ok</p>
<p>&nbsp;</p>
<h2><a name="Cluster-API-Exploration" id="Cluster-API-Exploration">Cluster API Exploration</a></h2>
<p>This section shows the Cluster APIs that I chose to look at</p>
<p>&nbsp;</p>
<h3><a name="Clusters-List" id="Clusters-List">Clusters List</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.databricks.com/api/latest/clusters.html#list">
https://docs.databricks.com/api/latest/clusters.html#list</a>, and this API call 
does the following:</p>
<ul>
	<li>Returns information about all pinned clusters, currently active 
	clusters, up to 70 of the most recently terminated interactive clusters in 
	the past 30 days, and up to 30 of the most recently terminated job clusters 
	in the past 30 days. For example, if there is 1 pinned cluster, 4 active 
	clusters, 45 terminated interactive clusters in the past 30 days, and 50 
	terminated job clusters in the past 30 days, then this API returns the 1 
	pinned cluster, 4 active clusters, all 45 terminated interactive clusters, 
	and the 30 most recently terminated job clusters.<br></li>
</ul>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.databricks.com/api/latest/clusters.html#list
        public async Task&lt;ClusterListResponse&gt; ClustersListAsync()
        {
            var request = new RestRequest("api/2.0/clusters/list", Method.GET);
            request.AddHeader("Authorization", $"Basic {_authHeader}");

            var response = await _client.ExecuteTaskAsync&lt;ClusterListResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;ClusterListResponse&gt;(response.Content);
            return dbResponse;
        }

        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>&nbsp;</p>
<h3><a name="Cluster-Get" id="Cluster-Get">Cluster Get</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.azuredatabricks.net/api/latest/clusters.html#get">
https://docs.azuredatabricks.net/api/latest/clusters.html#get</a>, and this API 
call does the following:</p>
<ul>
	<li>Retrieves the information for a cluster given its identifier. Clusters 
	can be described while they are running, or up to 60 days after they are 
	terminated</li>
</ul>
<p>&nbsp;</p>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.azuredatabricks.net/api/latest/clusters.html#get
        public async Task&lt;JObject&gt; ClustersGetAsync(string clusterId)
        {
            var request = new RestSharp.Serializers.Newtonsoft.Json.RestRequest("api/2.0/clusters/get", Method.GET);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddQueryParameter("cluster_id", clusterId);

            var response = await _client.ExecuteTaskAsync(request);
            JObject responseContent = JObject.Parse(response.Content);
            return responseContent;
        }

        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>&nbsp;</p>
<h3><a name="Cluster-Start" id="Cluster-Start">Cluster Start</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.azuredatabricks.net/api/latest/clusters.html#start">
https://docs.azuredatabricks.net/api/latest/clusters.html#start</a>, and this 
API call does the following:</p>
<ul>
	<li>Starts a terminated Spark cluster given its ID. This is similar to 
	createCluster, except:<ul>
		<li>The previous cluster id and attributes are preserved.</li>
		<li>The cluster starts with the last specified cluster size. If the 
		previous cluster was an autoscaling cluster, the current cluster starts 
		with the minimum number of nodes.</li>
		<li>If the cluster is not currently in a TERMINATED state, nothing will 
		happen.<br>Clusters launched to run a job cannot be started.</li>
	</ul>
	</li>
</ul>
<p>&nbsp;</p>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.azuredatabricks.net/api/latest/clusters.html#start
        public async Task&lt;DatabricksClusterStartResponse&gt; ClustersStartAsync(string clusterId)
        {
            var request = new RestSharp.Serializers.Newtonsoft.Json.RestRequest("api/2.0/clusters/start", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(new { cluster_id = clusterId });

            var response = await _client.ExecuteTaskAsync&lt;DatabricksClusterStartResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DatabricksClusterStartResponse&gt;(response.Content);
            return dbResponse;
        }
        
        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>&nbsp;</p>
<h2><a name="Jobs-API-Exploration" id="Jobs-API-Exploration">Jobs API Exploration</a></h2>
<p>This section shows the Jobs APIs that I chose to look at</p>
<p>&nbsp;</p>
<h3><a name="Jobs-List" id="Jobs-List">Jobs List</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.databricks.com/api/latest/jobs.html#list">
https://docs.databricks.com/api/latest/jobs.html#list</a>, and this API call 
does the following:</p>
<ul>
	<li>Lists all jobs<br></li>
</ul>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.databricks.com/api/latest/jobs.html#list
        public async Task&lt;JObject&gt; JobsListAsync()
        {
            var request = new RestSharp.Serializers.Newtonsoft.Json.RestRequest("api/2.0/jobs/list", Method.GET);
            request.AddHeader("Authorization", $"Basic {_authHeader}");

            var response = await _client.ExecuteTaskAsync(request);
            JObject responseContent = JObject.Parse(response.Content);
            return responseContent;
        }
		
		
        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3><a name="Jobs-Create" id="Jobs-Create">Jobs Create</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.databricks.com/api/latest/jobs.html#create">
https://docs.databricks.com/api/latest/jobs.html#create</a>, and this API call 
does the following:</p>
<ul>
	<li>Creates a new job with the provided settings<br></li>
</ul>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.databricks.com/api/latest/jobs.html#create
        public async Task&lt;CreateJobResponse&gt; JobsCreateAsync(string jsonJobRequest)
        {
            var request = new RestSharp.Serializers.Newtonsoft.Json.RestRequest("api/2.0/jobs/create", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddParameter("application/json", jsonJobRequest, ParameterType.RequestBody);
            var response = await _client.ExecuteTaskAsync&lt;CreateJobResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;CreateJobResponse&gt;(response.Content);
            return dbResponse;
        }
		
		
        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>An example request for this one is worth a special call out, as its not a simple parameter, we need to pass in quite a lot of JSON form this request. Here is an example 
request for a job that runs at 10:15pm each night:</p>
<pre lang="json">
{
  "name": "Nightly model training",
  "new_cluster": {
    "spark_version": "4.0.x-scala2.11",
    "node_type_id": "r3.xlarge",
    "aws_attributes": {
      "availability": "ON_DEMAND"
    },
    "num_workers": 10
  },
  "libraries": [
    {
      "jar": "dbfs:/my-jar.jar"
    },
    {
      "maven": {
        "coordinates": "org.jsoup:jsoup:1.7.2"
      }
    }
  ],
  "email_notifications": {
    "on_start": [],
    "on_success": [],
    "on_failure": []
  },
  "timeout_seconds": 3600,
  "max_retries": 1,
  "schedule": {
    "quartz_cron_expression": "0 15 22 ? * *",
    "timezone_id": "America/Los_Angeles"
  },
  "spark_jar_task": {
    "main_class_name": "com.databricks.ComputeModels"
  }
}
</pre>
<p>&nbsp;</p>
<p>Although the demo app doesn't use this one directly, I use a very similar one, 
which I will go through it quite some detail below.</p>
<h3><a name="&nbsp;" id="&nbsp;">&nbsp;</a></h3>
<h3><a name="Jobs-Runs-Get" id="Jobs-Runs-Get">Jobs Runs Get</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.databricks.com/api/latest/jobs.html#runs-get">
https://docs.databricks.com/api/latest/jobs.html#runs-get</a>, and this API call 
does the following:</p>
<ul>
	<li>Retrieves the metadata of a run<br></li>
</ul>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.databricks.com/api/latest/jobs.html#runs-get
        public async Task&lt;DatabricksRunResponse&gt; JobsRunsGetAsync(int runId)
        {
            var request = new RestRequest("api/2.0/jobs/runs/get", Method.GET);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddQueryParameter("run_id", runId.ToString());
            var response = await _client.ExecuteTaskAsync&lt;DatabricksRunResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DatabricksRunResponse&gt;(response.Content);
            return dbResponse;
        }
		
		
        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>&nbsp;</p>
<h3><a name="Jobs-Run-Now" id="Jobs-Run-Now">Jobs Run Now</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.databricks.com/api/latest/jobs.html#jobsrunnow">
https://docs.databricks.com/api/latest/jobs.html#jobsrunnow</a>, and this API call 
does the following:</p>
<ul>
	<li>Runs the job now, and returns the run_id of the triggered run<br></li>
</ul>
<p>This is simply done via the following code</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

		.....
		.....
		

        //https://docs.databricks.com/api/latest/jobs.html#jobsrunnow
        public async Task&lt;DatabricksRunNowResponse&gt; JobsRunNowAsync(DatabricksRunNowRequest runRequest)
        {
            var request = new RestRequest("api/2.0/jobs/run-now", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(runRequest);
            var response = await _client.ExecuteTaskAsync&lt;DatabricksRunNowResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DatabricksRunNowResponse&gt;(response.Content);
            return dbResponse;
        }
		
		
        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>Where we use this sort of request</p>
<pre lang="csharp">
using Newtonsoft.Json;

namespace SAS.Spark.Runner.REST.DataBricks.Requests
{
    // If you want to pass args you can do so using extra properties
    // See : https://docs.databricks.com/api/latest/jobs.html#run-now
    // - jar_params : 
    //   A list of parameters for jobs with jar tasks, e.g. "jar_params": ["john doe", "35"]. 
    //   The parameters will be used to invoke the main function of the main class specified 
    //   in the spark jar task. If not specified upon run-now, it will default to an empty list.
    // - notebook_params : 
    //   A map from keys to values for jobs with notebook task, 
    //   e.g. "notebook_params": {"name": "john doe", "age":  "35"}. 
    //   The map is passed to the notebook and will be accessible through the 
    //   dbutils.widgets.get function
    // - python_params :
    //   A list of parameters for jobs with python tasks, e.g. "python_params": ["john doe", "35"]. 
    //   The parameters will be passed to python file as command line parameters. 
    // If specified upon run-now, it would overwrite the parameters specified in job setting. 
    public class DatabricksRunNowRequest
    {
        [JsonProperty(PropertyName = "job_id")]
        public int JobId { get; set; }
    }
}
</pre>
<p>&nbsp;</p>
<h3><a name="Jobs-Run-Submit" id="Jobs-Run-Submit">Jobs Run Submit</a></h3>
<p> <a href="https://databricks.com/">Databricks</a> docs are here :
<a href="https://docs.azuredatabricks.net/api/latest/jobs.html#runs-submit">
https://docs.azuredatabricks.net/api/latest/jobs.html#runs-submit</a>, and this API call 
does the following:</p>
<ul>
	<li>Submit a one-time run with the provided settings. This endpoint doesn't 
	require a Databricks job to be created. You can directly submit your 
	workload. Runs submitted via this endpoint don't show up in the UI. Once the 
	run is submitted, you can use the <code>jobs/runs/get</code> API to check 
	the run state.<br></li>
</ul>
<p>Now this is probably the most complex, but most useful of ALL of the REST 
APIs, a it allows you to do the following :</p>
<ul>
	<li>Run by using a JAR writen Scala (where you can pass in command line args 
	too)</li>
	<li>Run using a notebook</li>
	<li>Run using a python file (where you can pass in command line args too)</li>
</ul>
<p>As I am quite keen on Scala I will be using Scala for the demo</p>
<p>&nbsp;</p>
<h4><a name="The-Scala-Project" id="The-Scala-Project">The Scala Project</a></h4>
<p>The demo code has a 2nd project in the source code : <code>
Src/SAS.SparkScalaJobApp</code> which is a IntelliJ IDEA Scala project. To run 
this you will need the prerequisites at the top of this article.</p>
<p>One you have downloaded the code you should run SBT in a command line window, 
and navigate to the&nbsp; <code>Src/SAS.SparkScalaJobApp</code> folder. And 
issue these SBT command</p>
<pre>&nbsp;
&gt; clean
&gt;compile
&gt;assembly
</pre>
<p>From there you should be able to go to the <code>Target </code>directory and see a FAT Jar 
(one with all dependencies bundled together)</p>
<p><img alt="" height="131" src="FatJar.png" width="630"></p>
<p>We will use this in just a moment, but lets just take a minute to examine the 
code. It is a very simple Spark job that expects a single Int command line 
argument (that we will send via the REST call in a moment), and will then create 
List of that many items that have some simple Spark transformations applied.</p>
<p>&nbsp;</p>
<div style="border:1px solid black; background-color:yellow">
<p style="margin:10px;"><strong>NOTE :</strong> </p>
	<p style="margin:10px;">&nbsp;</p>
	<p style="margin:10px;">One thing to note is that we need to be careful about how we use 
things like <code>SparkContext </code>and <code>SparkSession </code>which if you 
have done any spark before you will have created yourself. When using a Cloud 
provider such as AWS or Azure you need to use the existing <code>SparkContext
</code>and <code>SparkSession</code>, and we also need to avoid 
terminating/shutting down this items, as they are in effect shared. This blog is 
a good read on this :
<a href="https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html">
https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a></p>

</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<pre lang="scala">
import scala.util.Try
import scala.util.Success
import scala.util.Failure
import org.apache.spark.sql.SparkSession

object SparkApp extends App {

  println("===== Starting =====")

  if(args.length != 1) {
    println("Need exactly 1 int arg")
  }

  Try {
    Integer.parseInt(args(0))
  } match {
    case Success(v:Int) =&gt; {
      val combinedArgs = args.aggregate("")(_ + _, _ + _)
      println(s"Args were $combinedArgs")
      SparkDemoRunner.runUsingArg(v)
      println("===== Done =====")
    }
    case Failure(e) =&gt;  {
      println(s"Could not parse command line arg [$args(0)] to Int")
      println("===== Done =====")
    }
  }
}

object SparkDemoRunner {
  def runUsingArg(howManyItems : Int) : Unit =  {
    val session = SparkSession.builder().getOrCreate()
    import session.sqlContext.implicits._

    val multiplier = 2
    println(s"multiplier is set to $multiplier")

    val multiplierBroadcast = session.sparkContext.broadcast(multiplier)
    val data = List.tabulate(howManyItems)(_ + 1)
    val dataRdd = session.sparkContext.parallelize(data)
    val mappedRdd = dataRdd.map(_ * multiplierBroadcast.value)
    val df = mappedRdd.toDF
    df.show()
  }
}
</pre>
<p>&nbsp;</p>
<p>Anyway so once we have that Jar file available, we need to use a few APIs 
which I will go through 1 by 1, but here is the rough flow:</p>
<ul>
	<li>Examine if the chosen Jar file exists in the Dbfs (Databricks file 
	system, which means we have uploaded it already)</li>
	<li>Start the upload of the file (which we have to do in chunks as there is 
	a 1MB limit on the single
	<a href="https://docs.azuredatabricks.net/api/latest/dbfs.html#put">
	2.0/dbfs/put</a> API) to get a file handle</li>
	<li>Upload blocks of data for the file hadle as Base64 encoded strings</li>
	<li>Close the file using the file handle</li>
	<li>Craft a runs-submit request to make use of the just uploaded/latest Dbfs 
	file</li>
</ul>
<p>&nbsp;</p>
<p>So that's the rough outline of it</p>
<p>&nbsp;</p>
<p>So here is the ViewModel that will allow you pick the Jar (which as stated 
above should be in the <code>Target </code>folder of the&nbsp;&nbsp; <code>Src/SAS.SparkScalaJobApp</code> 
source code if you followed the instructions above to compile it using SBT.</p>
<pre lang="csharp">
using SAS.Spark.Runner.REST.DataBricks;
using System;
using System.IO;
using System.Threading.Tasks;
using System.Windows.Input;
using SAS.Spark.Runner.Services;
using System.Linq;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using System.Collections.Generic;
using Newtonsoft.Json;
using System.Diagnostics;

namespace SAS.Spark.Runner.ViewModels.Jobs
{
    public class JobsPickAndRunJarViewModel : INPCBase
    {
        private IMessageBoxService _messageBoxService;
        private IDatabricksWebApiClient _databricksWebApiClient;
        private IOpenFileService _openFileService;
        private IDataBricksFileUploadService _dataBricksFileUploadService;
        private string _jarFilePath;
        private string _status;
        private FileInfo _jarFile;
        private bool _isBusy;
        private bool _isPolling = false;
        private string _jobsJson;
        private Stopwatch _watch = new Stopwatch();

        public JobsPickAndRunJarViewModel(
            IMessageBoxService messageBoxService,
            IDatabricksWebApiClient databricksWebApiClient,
            IOpenFileService openFileService,
            IDataBricksFileUploadService dataBricksFileUploadService)
        {
            _messageBoxService = messageBoxService;
            _databricksWebApiClient = databricksWebApiClient;
            _openFileService = openFileService;
            _dataBricksFileUploadService = dataBricksFileUploadService;
            PickInputJarFileCommand = new SimpleAsyncCommand&lt;object, object&gt;(x =&gt; !IsBusy && !_isPolling, ExecutePickInputJarFileCommandAsync);
        }

        public string JobsJson
        {
            get
            {
                return this._jobsJson;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._jobsJson, value, () =&gt; JobsJson);
            }
        }

        public string JarFilePath
        {
            get
            {
                return this._jarFilePath;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._jarFilePath, value, () =&gt; JarFilePath);
            }
        }

        public string Status
        {
            get
            {
                return this._status;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._status, value, () =&gt; Status);
            }
        }

        public bool IsBusy
        {
            get
            {
                return this._isBusy;
            }
            set
            {
                RaiseAndSetIfChanged(ref this._isBusy, value, () =&gt; IsBusy);
            }
        }

        public ICommand PickInputJarFileCommand { get; private set; }
 
        

        private async Task&lt;object&gt; ExecutePickInputJarFileCommandAsync(object param)
        {
            IsBusy = true;

            try
            {
                _openFileService.Filter = "Jar Files (*.jar)|*.jar";
                _openFileService.InitialDirectory = @"c:\";
                _openFileService.FileName = "";
                var dialogResult = _openFileService.ShowDialog(null);
                if(dialogResult.Value)
                {
                    if(!_openFileService.FileName.ToLower().EndsWith(".jar"))
                    {
                        _messageBoxService.ShowError($"{_openFileService.FileName} is not a JAR file");
                        return Task.FromResult&lt;object&gt;(null);
                    }
                    _jarFile = new FileInfo(_openFileService.FileName);
                    JarFilePath = _jarFile.Name;
                    var rawBytesLength = File.ReadAllBytes(_jarFile.FullName).Length;
                    await _dataBricksFileUploadService.UploadFileAsync(_jarFile, rawBytesLength,
                        (newStatus) =&gt; this.Status = newStatus);

                    bool uploadedOk = await IsDbfsFileUploadedAndAvailableAsync(_jarFile, rawBytesLength);
                    if (uploadedOk)
                    {
                        //2.0/jobs/runs/submit
                        //poll for success using jobs/runs/get, store that in the JSON

                        var runId = await SubmitJarJobAsync(_jarFile);
                        if(!runId.HasValue)
                        {
                            IsBusy = false;
                            _messageBoxService.ShowError(this.Status = $"Looks like there was a problem with calling Spark API '2.0/jobs/runs/submit'");
                        }
                        else
                        {
                            await PollForRunIdAsync(runId.Value);
                        }

                    }
                    else
                    {
                        IsBusy = false;
                        _messageBoxService.ShowError("Looks like the Jar file did not upload ok....Boo");
                    }
                }
            }
            catch (Exception ex)
            {
                _messageBoxService.ShowError(ex.Message);
            }
            finally
            {
                IsBusy = false;
            }
            return Task.FromResult&lt;object&gt;(null);
        }

        private async Task&lt;bool&gt; IsDbfsFileUploadedAndAvailableAsync(FileInfo dbfsFile, int rawBytesLength)
        {
            bool fileUploadOk = false;
            int maxNumberOfAttemptsAllowed = 10;
            int numberOfAttempts = 0;

            while (!fileUploadOk || (numberOfAttempts == maxNumberOfAttemptsAllowed))
            {
                //check for the file in Dbfs
                var response = await _databricksWebApiClient.DbfsListAsync();
                fileUploadOk = response.files.Any(x =&gt;

                    x.file_size == rawBytesLength &&
                    x.is_dir == false &&
                    x.path == $@"/{dbfsFile.Name}"
                );
                numberOfAttempts++;
                this.Status = $"Checking that Jar has been uploaded ok.\r\nAttempt {numberOfAttempts} out of {maxNumberOfAttemptsAllowed}";
                await Task.Delay(500);
            }
            return fileUploadOk;
        }

        private async Task&lt;int?&gt; SubmitJarJobAsync(FileInfo dbfsFile)
        {
            this.Status = $"Creating the Spark job using '2.0/jobs/runs/submit'";

            // =====================================================================
            // EXAMPLE REQUEST
            // =====================================================================
            //{
            //  "run_name": "my spark task",
            //  "new_cluster": 
            //  {
            //    "spark_version": "3.4.x-scala2.11",
            //    "node_type_id": "Standard_D3_v2",
            //    "num_workers": 10
            //  },
            //  "libraries": [
            //    {
            //      "jar": "dbfs:/my-jar.jar"
            //    }
            //  ],
            //  "timeout_seconds": 3600,
            //  "spark_jar_task": {
            //    "main_class_name": "com.databricks.ComputeModels",
            //    "parameters" : ["10"]
            //  }
            //}

            var datePart = DateTime.Now.ToShortDateString().Replace("/", "");
            var timePart = DateTime.Now.ToShortTimeString().Replace(":", "");
            var request = new RunsSubmitJarTaskRequest()
            {
                run_name = $"JobsPickAndRunJarViewModel_{datePart}_{timePart}",
                new_cluster = new NewCluster
                {
                    // see api/2.0/clusters/spark-versions
                    spark_version = "4.0.x-scala2.11",
                    // see api/2.0/clusters/list-node-types
                    node_type_id = "Standard_F4s",
                    num_workers = 2
                },
                libraries = new List&lt;Library&gt;
                {
                    new Library { jar = $"dbfs:/{dbfsFile.Name}"}
                },
                timeout_seconds = 3600,
                spark_jar_task = new SparkJarTask
                {
                    main_class_name = "SparkApp",
                    parameters = new List&lt;string&gt;() { "10" }
                }
            };

            var response = await _databricksWebApiClient.JobsRunsSubmitJarTaskAsync(request);
            return response.RunId;
        }

        private async Task PollForRunIdAsync(int runId)
        {
            _watch.Reset();
            _watch.Start();
            while (_isPolling)
            {
                var response = await _databricksWebApiClient.JobsRunsGetAsync(runId);
                JobsJson = JsonConvert.SerializeObject(response, Formatting.Indented);
                var state = response.state;
                this.Status = "Job not complete polling for completion.\r\n" +
                    $"Job has been running for {_watch.Elapsed.Seconds} seconds";

                try
                {
                    if (!string.IsNullOrEmpty(state.result_state))
                    {
                        _isPolling = false;
                        IsBusy = false;
                        _messageBoxService.ShowInformation(
                            $"Job finnished with Status : {state.result_state}");
                    }
                    else
                    {
                        switch (state.life_cycle_state)
                        {
                            case "TERMINATING":
                            case "RUNNING":
                            case "PENDING":
                                break;
                            case "SKIPPED":
                            case "TERMINATED":
                            case "INTERNAL_ERROR":
                                _isPolling = false;
                                IsBusy = false;
                                break;
                        }
                    }
                }
                finally
                {
                    if (_isPolling)
                    {
                        await Task.Delay(5000);
                    }
                }
            }
        }
    }
}
</pre>
<p>Where we use this helper class to do the actual upload to Dbfs</p>
<pre lang="csharp">
using SAS.Spark.Runner.REST.DataBricks;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using System;
using System.IO;
using System.Linq;
using System.Threading.Tasks;

namespace SAS.Spark.Runner.Services
{
    public class DataBricksFileUploadService : IDataBricksFileUploadService
    {
        private IDatabricksWebApiClient _databricksWebApiClient;

        public DataBricksFileUploadService(IDatabricksWebApiClient databricksWebApiClient)
        {
            _databricksWebApiClient = databricksWebApiClient;
        }

        public async Task UploadFileAsync(FileInfo file, int rawBytesLength, 
            Action&lt;string&gt; statusCallback, string path = "")
        {
            var dbfsPath = $"/{file.Name}";

            //Step 1 : Create the file
            statusCallback("Creating DBFS file");
            var dbfsCreateResponse = await _databricksWebApiClient.DbfsCreateAsync(
                new DatabricksDbfsCreateRequest
                    {
                        overwrite = true,
                        path = dbfsPath
                    });

            //Step 2 : Add block in chunks
            FileStream fileStream = new FileStream(file.FullName, FileMode.Open, FileAccess.Read);
            var oneMegaByte = 1 &lt;&lt; 20;
            byte[] buffer = new byte[oneMegaByte];
            int bytesRead = 0;
            int totalBytesSoFar = 0;
            while ((bytesRead = fileStream.Read(buffer, 0, buffer.Length)) != 0)
            {
                totalBytesSoFar += bytesRead;
                statusCallback(
                    $"Uploaded {FormatAsNumeric(totalBytesSoFar)} " +
                    $"out of {FormatAsNumeric(rawBytesLength)} bytes to DBFS");
                var base64EncodedData = Convert.ToBase64String(buffer.Take(bytesRead).ToArray());

                await _databricksWebApiClient.DbfsAddBlockAsync(
                    new DatabricksDbfsAddBlockRequest
                        {
                            data = base64EncodedData,
                            handle = dbfsCreateResponse.Handle
                    });

            }
            fileStream.Close();

            //Step 3 : Close the file
            statusCallback($"Finalising write to DBFS file");
            await _databricksWebApiClient.DbfsCloseAsync(
                    new DatabricksDbfsCloseRequest
                    {
                        handle = dbfsCreateResponse.Handle
                    });

        }

        private string FormatAsNumeric(int byteLength)
        {
            return byteLength.ToString("###,###,###");
        }
    }
}
</pre>
<p>And just for completeness here is the set of REST APIs that make the 2 proceeded code snippets work</p>
<pre lang="csharp">
using System;
using System.Configuration;
using System.IO;
using System.Threading.Tasks;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using RestSharp;
using SAS.Spark.Runner.REST.DataBricks.Requests;
using SAS.Spark.Runner.REST.DataBricks.Responses;
using RestRequest = RestSharp.Serializers.Newtonsoft.Json.RestRequest;

namespace SAS.Spark.Runner.REST.DataBricks
{
    public class DatabricksWebApiClient : IDatabricksWebApiClient
    {
        private readonly string _baseUrl;
        private readonly string _authHeader;
        private readonly RestClient _client;

        public DatabricksWebApiClient()
        {
            _baseUrl = ConfigurationManager.AppSettings["BaseUrl"];
            _authHeader = Base64Encode(App.AccessToken);
            _client = new RestClient(_baseUrl);
        }

        //https://docs.azuredatabricks.net/api/latest/jobs.html#runs-submit
        public async Task&lt;DatabricksRunNowResponse&gt; JobsRunsSubmitJarTaskAsync(
			RunsSubmitJarTaskRequest runsSubmitJarTaskRequest)
        {
            var request = new RestRequest("2.0/jobs/runs/submit", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(runsSubmitJarTaskRequest);
            var response = await _client.ExecuteTaskAsync&lt;DatabricksRunNowResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DatabricksRunNowResponse&gt;(response.Content);
            return dbResponse;
        }

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#list
        public async Task&lt;DbfsListResponse&gt; DbfsListAsync()
        {
            var request = new RestRequest("api/2.0/dbfs/list", Method.GET);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddQueryParameter("path", "/");

            var response = await _client.ExecuteTaskAsync&lt;DbfsListResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DbfsListResponse&gt;(response.Content);
            return dbResponse;
        }

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#put
        public async Task&lt;JObject&gt; DbfsPutAsync(FileInfo file)
        {
            var request = new RestRequest("api/2.0/dbfs/put", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddFile("back", file.FullName);
            request.AddHeader("Content -Type", "multipart/form-data");

            var response = await _client.ExecuteTaskAsync(request);
            JObject responseContent = JObject.Parse(response.Content);
            return responseContent;
        }

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#dbfsdbfsservicecreate
        public async Task&lt;DatabricksDbfsCreateResponse&gt; DbfsCreateAsync(DatabricksDbfsCreateRequest dbfsRequest)
        {
            var request = new RestRequest("api/2.0/dbfs/create", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(dbfsRequest);

            var response = await _client.ExecuteTaskAsync&lt;DatabricksDbfsCreateResponse&gt;(request);
            var dbResponse = JsonConvert.DeserializeObject&lt;DatabricksDbfsCreateResponse&gt;(response.Content);
            return dbResponse;
        }

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#dbfsdbfsserviceaddblock
        public async Task&lt;JObject&gt; DbfsAddBlockAsync(DatabricksDbfsAddBlockRequest dbfsRequest)
        {
            var request = new RestRequest("api/2.0/dbfs/add-block", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(dbfsRequest);

            var response = await _client.ExecuteTaskAsync(request);
            JObject responseContent = JObject.Parse(response.Content);
            return responseContent;
        }

        //https://docs.azuredatabricks.net/api/latest/dbfs.html#close
        public async Task&lt;JObject&gt; DbfsCloseAsync(DatabricksDbfsCloseRequest dbfsRequest)
        {
            var request = new RestRequest("api/2.0/dbfs/close", Method.POST);
            request.AddHeader("Authorization", $"Basic {_authHeader}");
            request.AddJsonBody(dbfsRequest);

            var response = await _client.ExecuteTaskAsync(request);
            JObject responseContent = JObject.Parse(response.Content);
            return responseContent;
        }

        private static string Base64Encode(string plainText)
        {
            var plainTextBytes = System.Text.Encoding.UTF8.GetBytes(plainText);
            return Convert.ToBase64String(plainTextBytes);
        }
    }
}
</pre>
<p>As I say this is the most complex of all of the APIs I chose to look at. In 
reality you would probably not kick a <a href="https://databricks.com/">Databricks</a> 
job off from a UI. You might use a REST API of your own, which could delegate 
off to some Job manager like <a href="https://www.hangfire.io/">
https://www.hangfire.io/</a> which would obviously still have to do the polling 
part for you.&nbsp;</p>
<p>With all that in place you should be able to pick the JAR from the UI, and 
submit it, watch it run and see the logs from the <a href="https://databricks.com/">Databricks</a> 
web UI for the run.</p>
<p>&nbsp;</p>
<h1><a name="Conclusion" id="Conclusion">Conclusion</a></h1>
<p>I have to say using <a href="https://spark.apache.org/">Apache 
Spark</a>&nbsp; / <a href="https://databricks.com/">Databricks</a> is an 
absolute dream. <a href="https://databricks.com/">Databricks</a> have just 
nailed it, it's just what was needed, its awesome what you can do with it. 
Being able to spin up a cluster on demand that runs a job and is destroyed after 
the job run (to save the idle costs) is just frickin great.</p>
<p>I urge you to give it a look, I think you will love it</p>
<p>&nbsp;</p>










